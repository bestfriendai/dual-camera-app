Comprehensive Code Audit and Remediation Plan for DualLensPro Media PipelineTo: DualLensPro Development TeamFrom: Senior iOS Engineer, AVFoundation & Media Pipeline AuditDate: 10/28/2025Subject: Analysis and Correction of Critical Media Pipeline Failures (Rotation, Compositing, and Stability)Executive Summary: Identifying Core FailuresThis report details a comprehensive audit of the DualLensPro application code. The analysis confirms the user-reported issues: saved videos are incorrectly rotated, and a "green screen" artifact corrupts the majority of the video output (Images 2, 3, 4).These bugs are not separate; they are symptoms of two critical, interconnected failures within the RecordingCoordinator.swift actor. The application is suffering from a fundamental mathematical error in its video frame rotation logic, which leads to a buffer mismatch and the subsequent "green screen" rendering.Bug 1: The "Green Screen" (Buffer Mismatch): The "green screen" seen in Images 2, 3, and 4 is not a video feed error. It is the result of an uninitialized CVPixelBuffer. The AVAssetWriter is correctly configured to expect portrait (e.g., 1080x1920) video frames. However, the rotateAndMirrorPixelBuffer function in RecordingCoordinator.swift uses a mathematically incorrect CGAffineTransform.1 This transform renders the video outside the destination buffer's bounds. The CIContext therefore writes nothing into the buffer. This empty buffer, which uses the kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange (YUV) pixel format 1, is then passed to the writer. A YUV buffer with zeroed luma and chroma planes is interpreted by video decoders as solid green.2Bug 2: The 90-Degree Rotation (Incorrect Math): The developer correctly identified that the landscape buffers from the camera sensor must be rotated to create the desired portrait video. However, the transform CGAffineTransform(rotationAngle:.pi / 2) 1 applies a 90-degree counter-clockwise (CCW) rotation.5 iOS camera sensors typically require a 90-degree clockwise (CW) rotation to appear upright in portrait mode.6 This error is why the partial video that is visible in Image 4 is sideways.Bug 3: The Compositing Layout (UI Mismatch): The live preview in Image 1 clearly shows the Back Camera on top and the Front Camera on the bottom. The FrameCompositor.swift file 1 is coded with the inverse logic, placing the Front Camera on top. This must be corrected to match the user's expectation.Bug 4: Application Instability (Session Reconfiguration): A developer comment, // FORCE video mode for now to prevent crashes 1, found in CameraViewModel.swift confirms the app is unstable when switching capture modes. This is a common and critical bug related to improperly reconfiguring an AVCaptureMultiCamSession on the main thread.8This report provides the precise code-level fixes for all four issues, starting with the most critical rotation and "green screen" bugs.Section 1: Fixing the "Green Screen" and Rotation BugThe root of the media corruption is the rotateAndMirrorPixelBuffer function in RecordingCoordinator.swift.1 This function is called for every video frame for all three recorders (front, back, and combined). Its flawed logic is the source of both the green screen and the incorrect rotation.Analysis of the Flawed TransformThe current implementation attempts to perform rotation using manual CGAffineTransform math:Swift// --- BROKEN CODE From RecordingCoordinator.swift ---
let rotateTransform = CGAffineTransform(rotationAngle:.pi / 2) // 90-deg CCW
   .translatedBy(x: 0, y: -CGFloat(inputWidth)) // inputWidth = 1920
ciImage = ciImage.transformed(by: rotateTransform)
//...
let outputRect = CGRect(x: 0, y: 0, width: dimensions.width, height: dimensions.height) // 1080x1920
context.render(ciImage, to: output, bounds: outputRect,...)
This code fails for two reasons:Incorrect Rotation Direction: As noted, .pi / 2 is a 90-degree counter-clockwise (CCW) rotation.5 This is the wrong direction.Incorrect Transform Math (The "Green Screen"): A CGAffineTransform rotates around the origin (0,0) of the coordinate system. When the 1920x1080 buffer is rotated 90° CCW and translated, its new coordinates are rendered entirely in the negative space (e.g., an extent of roughly CGRect(x: -1080, y: -1920, width: 1080, height: 1920)).The context.render call 1 then attempts to render this image (which exists at x: -1080) into the output buffer, which exists at CGRect(x: 0, y: 0, width: 1080, height: 1920). There is no overlap between the image's extent and the buffer's render rectangle. The CIContext renders nothing, leaving the YUV buffer uninitialized and resulting in the solid green output.2The Robust Solution: Pixel-Level Rotation with CIImage.orientedManual 2D transform math should be avoided. The CoreImage framework provides a simple, highly optimized method for this exact task. The rotateAndMirrorPixelBuffer function must be completely replaced with the following implementation.Corrected Code: RecordingCoordinator.swiftThis function correctly uses .oriented(.right) to perform the 90-degree clockwise rotation and then applies a standard mirror transform.Swift/// Rotates and optionally mirrors a pixel buffer from landscape (1920x1080) to portrait (1080x1920)
private func rotateAndMirrorPixelBuffer(_ pixelBuffer: CVPixelBuffer, to dimensions: (width: Int, height: Int), mirror: Bool) -> CVPixelBuffer? {
    guard let context = ciContext else {
        print("❌ No CIContext for rotation")
        return nil
    }

    // 1. Create CIImage from the landscape CVPixelBuffer
    var ciImage = CIImage(cvPixelBuffer: pixelBuffer)

    // 2. Apply a 90-degree CLOCKWISE rotation.
    //.right corresponds to EXIF orientation 6, which is 90-deg CW.
    // This correctly transforms the 1920x1080 (landscape) image
    // to a 1080x1920 (portrait) image with its origin at (0,0).
    ciImage = ciImage.oriented(.right)

    // 3. Apply a horizontal mirror transform if required (for front camera)
    if mirror {
        // A mirror is a horizontal scale by -1.
        // This must be translated back into bounds by the image's new width (1080).
        let mirrorTransform = CGAffineTransform(scaleX: -1, y: 1)
           .translatedBy(x: -ciImage.extent.width, y: 0)
        ciImage = ciImage.transformed(by: mirrorTransform)
    }

    // 4. Create the output pixel buffer with PORTRAIT dimensions
    var outputBuffer: CVPixelBuffer?
    let status = CVPixelBufferCreate(
        kCFAllocatorDefault,
        dimensions.width,  // e.g., 1080
        dimensions.height, // e.g., 1920
        kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange,
       ,
            kCVPixelBufferMetalCompatibilityKey: true
        ] as CFDictionary,
        &outputBuffer
    )

    guard status == kCVReturnSuccess, let output = outputBuffer else {
        print("❌ Failed to create rotated pixel buffer")
        return nil
    }

    // 5. Render the correctly-rotated image into the output buffer
    // The.oriented() image now has a correct extent of (0, 0, 1080, 1920),
    // which perfectly matches the output buffer and rect.
    let outputRect = CGRect(x: 0, y: 0, width: dimensions.width, height: dimensions.height)
    context.render(ciImage, to: output, bounds: outputRect, colorSpace: CGColorSpaceCreateDeviceRGB())

    return output
}
This single function replacement will fix both the green screen artifact and the 90-degree rotation for all three saved video files.Table 1: Rotation Transform Logic ComparisonTo further clarify the "research behind it," the following table contrasts the flawed logic with the correct implementation.MethodCode Snippet (Swift)90° RotationResulting Image Extent (x, y, w, h)OutcomeA: As-Is (Broken)CGAffineTransform(rotationAngle:.pi / 2).translatedBy(x: 0, y: -1920)Counter-Clockwise (CCW) 5(-1080, -1920, 1080, 1920)BUG: Image is rendered entirely outside the buffer's (0, 0, 1080, 1920) bounds. This results in an uninitialized (green) buffer.2B: Corrected MathCGAffineTransform(rotationAngle: -.pi / 2).translatedBy(x: 1080, y: 0)Clockwise (CW) 12(0, 0, 1080, 1920)Correct (but complex): Image is correctly rotated and fills the buffer. This is fragile and hard to maintain.C: Recommended FixCIImage(cvPixelBuffer:...).oriented(.right)Clockwise (CW) 13(0, 0, 1080, 1920)Correct (and simple): Uses the framework's built-in, optimized method for 90-degree CW rotation (EXIF 6).15 This is the most robust and idiomatic fix.Section 2: Optimizing for Production (Metadata Transforms)The fix in Section 1 is correct, but inefficient. It performs a heavy, pixel-level rotation on the GPU for every single frame for all three video writers.16 This consumes significant battery and thermal headroom, which can lead to frame drops or device overheating during long recordings.The professional-grade solution is to use metadata transforms. This approach involves:Writing the original, landscape (1920x1080) buffers directly to the AVAssetWriter.Attaching a metadata flag (a CGAffineTransform) to the video track that instructs the video player (e.g., QuickTime, Photos app) to "rotate this 90 degrees during playback".6This method offloads all rotation work to the player, making the recording process itself extremely efficient.Implementation CaveatThis metadata-only approach cannot be fully implemented in the current app architecture. The FrameCompositor.swift 1 requires portrait-oriented (1080x1920) buffers to stack them vertically for the merged video.Recommended Hybrid SolutionA hybrid approach provides the best balance of efficiency and functionality:Individual Files (Front/Back): Use metadata transforms. These writers will be configured for landscape video and have a transform applied. This is highly efficient.Merged File (Combined): Continue to use pixel-level rotation. This writer will be configured for portrait video and will be fed the rotated buffers from the function fixed in Section 1.This change would be implemented in RecordingCoordinator.configure:Configure frontVideoInput and backVideoInput with landscape settings (e.g., 1920x1080).Apply a 90-degree metadata transform to these inputs: frontVideoInput?.transform = my90DegreeCWTransform.18Configure combinedVideoInput with portrait settings (e.g., 1080x2160) and an identity transform: combinedVideoInput?.transform =.identity.Modify appendFrontPixelBuffer and appendBackPixelBuffer to:Append the original landscape buffer to frontPixelBufferAdaptor / backPixelBufferAdaptor.Append the rotated portrait buffer (from rotateAndMirrorPixelBuffer) to combinedPixelBufferAdaptor (via the compositor).This optimization saves 2/3 of the GPU workload during recording and is the recommended architecture for a production-level app. For simplicity, the fix in Section 1 (rotating all three) is also acceptable but less performant.Section 3: Correcting the Merged Video LayoutThe live preview (Image 1) shows the Back Camera on top and the Front Camera on the bottom. The FrameCompositor.swift 1 file is coded to produce the opposite.Analysis of FrameCompositor.stackedBuffersThe current compositing logic is:Swift// --- BROKEN CODE From FrameCompositor.swift ---
// This translates the FRONT camera to the top half
let frontPositioned = frontScaled.transformed(by: CGAffineTransform(translationX: 0, y: halfHeight))
// This leaves the BACK camera at the bottom
let backPositioned = backScaled
// This composites the (top) front camera OVER the (bottom) back camera
let composed = frontPositioned.composited(over: backPositioned)
In the CoreImage coordinate system, Y=0 is the bottom of the buffer. By translating frontScaled by y: halfHeight, the code places the front camera in the top half of the frame. This is the inverse of the UI in Image 1.The Fix (Inverting the Layout)The logic must be swapped. The back camera should be translated to the top, and the front camera should be at the bottom.Corrected Code: FrameCompositor.swiftSwiftprivate func stackedBuffers(front: CVPixelBuffer, back: CVPixelBuffer) -> CVPixelBuffer? {
    guard let outputBuffer = allocatePixelBuffer() else {
        print("❌ FrameCompositor: Failed to allocate output buffer")
        return nil
    }

    let frontImage = CIImage(cvPixelBuffer: front)
    let backImage = CIImage(cvPixelBuffer: back)

    let outputWidth = CGFloat(width)
    let outputHeight = CGFloat(height)
    let halfHeight = outputHeight / 2

    // Scale both images to fill their half of the screen
    let frontScaled = scaleToFit(image: frontImage, width: outputWidth, height: halfHeight)
    let backScaled = scaleToFit(image: backImage, width: outputWidth, height: halfHeight)

    // --- THIS IS THE FIX ---

    // 1. Translate the BACK camera to the TOP half (y = halfHeight)
    let backPositioned = backScaled.transformed(by: CGAffineTransform(translationX: 0, y: halfHeight))
    
    // 2. Keep the FRONT camera at the BOTTOM half (y = 0)
    let frontPositioned = frontScaled 

    // 3. Composite the (top) back camera OVER the (bottom) front camera
    let composed = backPositioned.composited(over: frontPositioned)
    
    // --- END FIX ---

    // Render the final composed image to the output buffer
    let outputRect = CGRect(x: 0, y: 0, width: width, height: height)
    context.render(composed, to: outputBuffer, bounds: outputRect, colorSpace: CGColorSpaceCreateDeviceRGB())

    return outputBuffer
}
Section 4: Resolving Application InstabilityThe application code exhibits two other significant issues that compromise stability and maintainability.1. The Capture Mode Switching CrashA developer comment in CameraViewModel.swift states: // FORCE video mode for now to prevent crashes.1 This indicates a known instability when switching between capture modes.This is a common and severe bug when using AVCaptureMultiCamSession. The CaptureMode enum 1 defines modes that require different hardware outputs (e.g., .photo requires AVCapturePhotoOutput, while .video requires AVCaptureVideoDataOutput). Switching these outputs on a live session is a dangerous operation that can easily cause a crash if not handled correctly.8The cardinal rules for reconfiguring an AVCaptureSession are:All configuration changes must be bracketed by session.beginConfiguration() and session.commitConfiguration().10These changes must be dispatched to the serial sessionQueue to prevent race conditions, not run on the main thread.Recommended Code: DualCameraManager.swiftThe following function should be added to the DualCameraManager class to handle this transition safely.Swift// Add this function to your DualCameraManager.swift file
// Assumes you have:
// private let sessionQueue = DispatchQueue(label: "com.duallens.sessionQueue")
// private let session = AVCaptureMultiCamSession()
// private var videoDataOutput = AVCaptureVideoDataOutput()
// private var photoOutput = AVCapturePhotoOutput()

func setCaptureMode(_ newMode: CaptureMode) {
    // Perform all session changes on the dedicated session queue
    sessionQueue.async { [weak self] in
        guard let self = self, self.isSessionRunning else { return }

        // 1. Lock the session for configuration
        self.session.beginConfiguration()
        defer { self.session.commitConfiguration() } // 4. Commit changes

        // 2. Remove all existing outputs
        if self.session.outputs.contains(self.videoDataOutput) {
            self.session.removeOutput(self.videoDataOutput)
        }
        if self.session.outputs.contains(self.photoOutput) {
            self.session.removeOutput(self.photoOutput)
        }

        // 3. Add the correct output for the new mode
        if newMode.isRecordingMode { //.video,.action
            if self.session.canAddOutput(self.videoDataOutput) {
                self.session.addOutput(self.videoDataOutput)
                print("✅ DualCameraManager: Switched to Video Mode")
            } else {
                print("❌ DualCameraManager: Failed to add VideoDataOutput")
            }
        } else if newMode.isPhotoMode { //.photo,.groupPhoto
            if self.session.canAddOutput(self.photoOutput) {
                self.session.addOutput(self.photoOutput)
                print("✅ DualCameraManager: Switched to Photo Mode")
            } else {
                print("❌ DualCameraManager: Failed to add PhotoOutput")
            }
        }
        // Note:.switchScreen is a UI-only action and does not require session reconfiguration
    }
}
After adding this, the // FORCE video mode comment in CameraViewModel.swift can be safely removed.2. Redundant Info.plist FilesThe project structure 1 shows two Info.plist files for the main application:DualLensPro/Info.plistDualLensPro/DualLensPro/Info.plistThis is a project configuration error that leads to confusion and potential build conflicts.21 The app target can only have one Info.plist file. The second file (DualLensPro/DualLensPro/Info.plist) contains the privacy descriptions and is the one actively being used. The first file is likely an unused default.The Fix (Project Housekeeping)In Xcode, navigate to your Project Settings (click the "DualLensPro" project icon in the navigator).Select the DualLensPro Target.Go to the Build Settings tab.In the search bar, type Info.plist File.Verify the path under "Packaging" points to the correct file: DualLensPro/DualLensPro/Info.plist.21Right-click the other Info.plist (at DualLensPro/Info.plist) in the Project Navigator.Select "Delete" and choose "Move to Trash".Section 5: Final Remediation ChecklistTo resolve all identified issues, the following actions must be taken.File to EditAction RequiredRecordingCoordinator.swiftReplace the entire rotateAndMirrorPixelBuffer function with the correct, CIImage.oriented(.right)-based implementation from Section 1.FrameCompositor.swiftModify the stackedBuffers function to invert the compositing logic, placing the back camera on top, as shown in Section 3.DualCameraManager.swiftAdd the robust, queue-safe setCaptureMode(_ newMode: CaptureMode) function from Section 4 to fix the mode-switching crash.CameraViewModel.swiftRemove the // FORCE video mode for now to prevent crashes comment 1 and ensure cameraManager.setCaptureMode(mode) is called when currentCaptureMode changes.Project File (.xcodeproj)Delete the redundant DualLensPro/Info.plist file after verifying the correct path in Build Settings, as detailed in Section 4.Completing these five steps will resolve the critical "green screen" bug, correct the video rotation and compositing layout, and stabilize the application for production use.

Fixing Dual-Camera Video Orientation and Green Screen Issues in iOS
Introduction
Dual-camera recording apps allow simultaneous capture from the front and back cameras, producing separate video files and a combined stacked view (front on top of back). Ensuring all output videos are in portrait orientation, properly stacked (front-top, back-bottom), and free of green screen artifacts requires careful handling of the AVFoundation pipeline. This document outlines best practices and code-level fixes for a Swift-based dual-camera app (tested on iPhone 17 Pro Max, iOS 26) to achieve consistent, artifact-free results across devices. We will cover multi-camera capture setup, pixel buffer and AVAssetWriter configuration, Core Image rendering with correct color space, orientation transforms, frame timing (pipeline flushing), and debugging techniques.
Multi-Camera Capture Session Setup
Modern iOS devices support simultaneous capture from multiple cameras via AVCaptureMultiCamSession (introduced in iOS 13). Using a multi-cam session is essential for recording front and back cameras concurrently. First, check device support with AVCaptureMultiCamSession.isMultiCamSupported (or a similar flag) and fall back gracefully if not supported
stackoverflow.com
. On supported devices, configure a single AVCaptureMultiCamSession and add two camera inputs (front and rear) and corresponding outputs. For example, add two AVCaptureVideoDataOutput (or AVCaptureMovieFileOutput) instances – one for each camera – plus an AVCaptureAudioDataOutput for the microphone. Ensure you configure capture presets and formats so that both camera outputs use the same resolution and frame rate (e.g. 1080p @ 30fps) to simplify synchronization. Apple historically limited multi-cam to 1080p on many devices for performance, so choose a resolution that both cameras and the device can handle simultaneously. Performance considerations: Multi-camera capture is resource intensive. Limit the session preset (e.g. .hd1920x1080) and frame rate to what the hardware can sustain. Use .videoHD formats rather than 4K unless the device (like newer iPhones) explicitly supports dual 4K capture. Also disable unnecessary outputs (e.g. depth or metadata outputs) to conserve bandwidth. Activating both cameras draws more power and can generate heat, so handle interruptions (thermal pressure or app backgrounding) appropriately. Ensure the session is started (session.startRunning()) only after all inputs/outputs are added and configuration is successful (checking return values of addInput/addOutput). Use separate sample buffer delegate queues for each camera output to parallelize processing, but be mindful of thread safety when merging frames. Finally, keep the capture session running while recording all streams. Monitor AVCaptureSessionRuntimeError or interruption notifications – for instance, if one camera fails to deliver, you may need to stop the session or restart. Logging the session status (e.g. isRunning, multiCamSupported flags) is helpful for debugging. In summary, using the proper multi-cam API and session setup provides the foundation for capturing dual videos concurrently in a stable manner.
AVAssetWriter and Pixel Buffer Configuration
After capturing frames from both cameras, the app uses AVAssetWriter to encode three movie files (front, back, combined). Proper configuration of AVAssetWriterInput and AVAssetWriterInputPixelBufferAdaptor is critical to avoid orientation and green-frame problems. Video settings: Initialize each AVAssetWriterInput with output settings matching the desired portrait dimensions. For the individual front/back videos, use a portrait orientation resolution (width < height). For example, if the camera native capture is 1920×1080 (landscape), set the writer to 1080×1920 for portrait output. For the combined video, which stacks two frames vertically, double the height – e.g. 1080×3840 if stacking two 1080×1920 buffers. Using explicit portrait dimensions ensures the video is actually encoded upright rather than relying on metadata rotation. Choose an appropriate codec (HEVC or H.264) and increase the bitrate for the combined video since it has twice the pixels (in the example code, the combined output uses roughly 2× the bitrate of individual videos to maintain quality). Set each video AVAssetWriterInput’s expectsMediaDataInRealTime to true for live capture. Orientation transform: Avoid using the AVAssetWriterInput.transform property to rotate the video, as this can complicate multi-stream synchronization. Instead, leave the transform as identity for all writers and handle rotation in the pixel buffer processing (discussed in the next section). In the sample code, transforms were explicitly set to .identity (removing any prior rotation) because the pixel buffers are pre-rotated to portrait in software. Pixel buffer adaptor: For each video input, create an AVAssetWriterInputPixelBufferAdaptor to append CVPixelBuffer frames. It’s critical to specify the adaptor’s source pixel buffer attributes to match the actual pixel format and frame size of your buffers. Include keys for pixel format (e.g. kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange for NV12), width, height, and empty IOSurface properties. Defining these attributes ensures the writer creates an optimal CVPixelBufferPool. If attributes are omitted or incorrect, the video may encode incorrectly (often resulting in a solid green frame or corrupted image)
stackoverflow.com
. For instance, one developer found that leaving the adaptor attributes nil led to an all-green video, whereas explicitly providing them fixed the issue
stackoverflow.com
. In our case, we use portrait width/height in the attributes for front/back buffers, and the doubled height for the combined buffer, exactly matching each output’s resolution. After setting up the writers and adaptors, call startWriting() on each AVAssetWriter and startSession(atSourceTime:) (using a common timestamp, typically the first captured frame’s timestamp) before appending buffers. The adaptor’s underlying pixel buffer pool will be nil until after starting the session
stackoverflow.com
, so allocate pixel buffers only after the session has begun. Throughout recording, check AVAssetWriterInput.isReadyForMoreMediaData before appending; if an input isn’t ready (encoder is backed up), dropping or delaying frames may be necessary to avoid stalls.
Video Orientation and Transform Handling
To ensure all videos save in portrait orientation, handle rotation at the buffer level. Camera sensors and AVCaptureVideoDataOutput typically deliver frames in a default orientation (landscape, width > height). In a portrait recording scenario, the raw buffers need to be rotated. There are two approaches: using capture connection orientation or rotating the pixel buffers manually. Apple’s older API allowed setting AVCaptureConnection.videoOrientation, but as of iOS 17 this is deprecated in favor of using a AVCaptureDevice.RotationCoordinator or adjusting videoRotationAngle
stackoverflow.com
. In a multi-camera context, it's simpler to manually rotate the image buffers using Core Image or Accelerate, since we are also compositing frames. Manual rotation: In the sample implementation, each incoming frame is rotated to portrait via Core Image before encoding. The app determines the device orientation at start (portrait vs landscape) and whether a camera is front-facing (which influences mirroring). For each frame from the front camera, we apply a 90° clockwise rotation (if recording in portrait orientation) and then a horizontal flip (mirror) so that the front video appears as a “selfie” view. The back camera frames get a 90° rotation but no mirror. This can be done by creating a CIImage from the CVPixelBuffer and using oriented(.right) for rotation, plus a flip transform for mirroring (scaleX = -1). These operations produce a correctly oriented CIImage. We then render that into a new CVPixelBuffer (from the adaptor’s pool) which has the portrait dimensions. Each rotated buffer is then appended to the respective AVAssetWriterInputPixelBufferAdaptor. With this approach, the video files themselves are encoded upright, and no further metadata rotation is needed. The code comments highlight this fix: “Set transforms to .identity. We are rotating pixels manually.”. An alternative approach is to rely on AVAssetWriterInput.transform to tag the video track with a rotation. However, that can complicate the combined video case and preview synchronization. It’s more straightforward to produce the final orientation in the pixel data. Note: If you do use the new iOS 17+ videoRotationAngle API for photos or other captures, be mindful that it expects degrees (0–360) and must be used in coordination with a rotation coordinator to handle device tilt
stackoverflow.com
. For video data outputs, typically you will handle rotation manually as described.
Real-Time Core Image Composition (Stacked Layout)
To create the combined stacked video (front-top, back-bottom), use a real-time image compositing pipeline. The app can leverage Core Image (CI) for GPU-accelerated merging of the two camera streams. Each time a new pair of frames arrives (one from either camera), the compositor should produce a single merged frame. A robust approach is to always use the latest frame from each camera; if one camera’s frame is late, reuse the previous frame from that camera to avoid gaps (the example caches the last front frame and uses it when only a back frame arrives). During the final stages of recording (shutdown), this cache is disabled to prevent using stale frames – only complete pairs are composed to avoid any frozen half-frame in the output. Stacking implementation: Convert the front and back CVPixelBuffer into CIImage objects. Decide the output resolution for the combined image (we use width = e.g.1080, height = 3840 for two 1920-high frames stacked). Using Core Image, scale each camera image to fit exactly half of the vertical output. In code, a helper scaleToFit(image, width, height) is used to aspect-fill the image into a target size, then center-crop. We obtain two CIImages: one resized to full width × half-height for front, and similarly for back. Next, apply a vertical translation to position the front image at the top of a blank canvas – e.g. translate by +halfHeight so the front occupies the top region. The back image remains at the origin (bottom). Then composite the front image over the back image (using CIImage.composited(over:)) to form one combined image. Finally, render the composed CIImage into a new pixel buffer. Important: Use a single CIContext for all rendering and specify the bounds and color space when rendering to a pixel buffer. For example:
let outputRect = CGRect(x: 0, y: 0, width: outputWidth, height: outputHeight)
context.render(composedImage, to: outputPixelBuffer, bounds: outputRect, colorSpace: CGColorSpaceCreateDeviceRGB())
In testing, explicitly providing the destination color space (such as Device RGB) and bounds prevented green-tinted frames. This ensures Core Image knows how to convert the composite into the pixel buffer’s format. The pixel buffer in our case is YUV (NV12), and without specifying a color space Core Image might default to a extended linear RGB working space; the mismatch can cause a green or purple hue or even an entirely green frame. By matching to DeviceRGB (standard sRGB), the CIContext will correctly translate the colors into the YUV buffer. (The working color space for the CIContext can also be set to sRGB; in code we initialized the CIContext with .workingColorSpace: CGColorSpaceCreateDeviceRGB() as well to be safe.) The composition and render should be done on a background thread or within the camera capture callback, ensuring it can keep up with frame rate. Because CI utilizes GPU, these calls are asynchronous internally – meaning the render may be enqueued on GPU and complete shortly after the call returns. We’ll address synchronization of the GPU pipeline in the next section.
Frame Timing and Preventing Green Screen Artifacts
Presentation timestamps: Maintain proper timing when writing frames to avoid buffer mis-order that can corrupt the video. Use the CMSampleBuffer’s timestamp for each frame as the presentation time for append(pixelBuffer, withPresentationTime:). Ensure the front and back frames are timestamped from the same clock (in an AVCaptureMultiCamSession, frames from both outputs use a synchronized timeline). In our implementation, when a back camera frame arrives, we compose it with the most recent front frame and use the back frame’s timestamp for the combined output. This means the combined video frame rate will follow the back camera’s cadence, using a cached front image if the front frame isn’t available at that exact moment. This strategy keeps the combined video time-aligned and avoids duplication when one camera lags slightly. Always check that timestamps are monotonically increasing; a common pitfall is reusing a timestamp or using the wrong time base which can make AVAssetWriter complain or drop frames. Writer readiness: Monitor isReadyForMoreMediaData on each AVAssetWriterInput. If an input isn’t ready (encoder is busy), and you still append a frame, the call will return false and that frame will be dropped. The code logs a warning if append returns false so you can detect if frames are being lost. Occasional drops might happen under heavy load (e.g. high FPS or slow codec). If too many drops occur, consider reducing frame rate or simplifying processing. Additionally, ensure you append video and audio samples in increasing PTS order. Audio samples can be appended to all files (front, back, combined) using the same timestamp from the audio buffer – this keeps audio in sync across videos. Flushing the GPU pipeline: One tricky cause of final-frame glitches or green flashes is the asynchronous nature of GPU rendering. When you stop recording, there may be CI render operations still in flight on the GPU. If you immediately finalize the AVAssetWriter (calling finishWriting()), the last few frames might not be fully rendered and will appear as solid green or corrupted in the output. To prevent this, flush the GPU pipeline before finishing. A known technique is to perform a dummy render that forces the GPU queue to empty. For example, render a blank CIImage (or simply call context.flush() if available). In our case, we create an empty CIImage and render it to a throwaway pixel buffer once recording stops, which effectively blocks until the GPU has completed all prior draws. After this flush, add a slight delay on the thread (e.g. 50 ms) to give any remaining tasks time to complete. Only then call finishWriting on the asset writers. This sequence ensures the last composite frame has actually been written to the file before closing. The code marks this as a critical fix: “Flush GPU render pipeline to ensure all pending renders complete”. Additionally, flush and reset any pixel buffer pools if needed. The sample uses CVPixelBufferPoolFlush(pool, 0) when the FrameCompositor deinitializes to release any cached buffers. This isn’t directly related to green frames, but is a good cleanup practice to avoid memory leaks or reuse of invalid buffers.
Debugging Layout and Rendering Issues
When fixing layout/orientation and green screen issues, use a systematic debugging approach:
Verify inputs and outputs: Print out the resolutions and orientations of incoming buffers (e.g. using CVPixelBufferGetWidth/Height). Ensure they match expectations (e.g. you think you’re getting 1920x1080 but actually get a different size). Mismatches can indicate a configuration issue. The app’s logs can show when the session is running and multi-cam is supported, and also log frame processing steps. If possible, log every Nth frame’s timestamp to ensure both cameras are delivering frames consistently.
Check the composition visually: During development, you can render a few combined frames to an image and inspect them (for example, pause the app and grab a pixel buffer to see if front/back images overlap correctly). The provided code’s composition functions (stackedBuffers, pictureInPicture) can be unit-tested or run with sample pixel data. Look for any half frames or incorrect layering. A common mistake is forgetting to clear the output pixel buffer background – if one image doesn’t cover the entire frame, the uncovered region could carry over from a previous frame or be undefined (often appearing green). The solution is to always draw the bottom layer first covering full frame. In our case, we scale the back image to fill the full frame, so it acts as a background, and thus no region is left uninitialized.
Color tint or green overlay: If the combined video has a green tint or a translucent green layer, it's likely a color space or alpha issue. Ensure you’re not inadvertently using an alpha channel improperly – e.g. composited(over:) in CI respects alpha; if your images lack alpha, they are treated as fully opaque (which is what we want). If one image had an alpha and wasn't premultiplied, it could blend with a green background (green is the default clear color in some contexts). Setting .outputPremultiplied: true in CIContext (as we did) and working in RGB avoids that. Also, double-check that all CIImages are created from CVPixelBuffer without color space issues (use CIImage(cvPixelBuffer:) which should infer the pixel buffer's color space). Specifying color space on render (as discussed) solves most of these issues.
Orientation issues: If the front/back videos or the combined video appear rotated or mirrored incorrectly when played back, revisit the transform vs. rotation logic. A good test is to record a scene with obvious orientation (e.g. text or a sign in the background) to see if the saved front/back videos are truly upright. If not, you may have missed applying the rotation to the buffer, or if you relied on writer transforms, those might not be honored by all players. The recommended fix (as implemented) is to rotate the pixel buffers via Core Image (or using vImageRotate for a CPU approach) and keep writer transform at identity. For mirroring, ensure the flip is applied exactly once – a common bug is accidental double-mirroring if both preview layer and output apply a mirror. In our pipeline, the preview might mirror the front camera for UI, but we still mirror again for the recorded output to maintain the “selfie” perspective in the saved video.
Frame drops or timing: If the combined video stutters or has mismatched audio, check the timestamps and writer readiness as noted. The logging in append(pixelBuffer:) will warn if frames fail to append. Also monitor the console for any AVFoundation warnings. If you see **Session did drop sample buffer** messages, the device might not keep up – consider reducing quality or investigating if any configuration (like enabling video stabilization on both cameras) is overloading the pipeline. For multi-cam, Apple recommends disabling camera stabilization or other heavy processing on at least one camera to save processing power.
Using the above techniques, iterate on the implementation until the output videos consistently show correct orientation and synchronized content, with no green frames from start to end.
iOS 26 and Future AVFoundation Considerations
As iOS evolves (iOS 26 and beyond), Apple has introduced changes that developers should adapt to:
Orientation API changes: As mentioned, AVCaptureConnection.videoOrientation (and isVideoMirrored) were deprecated in iOS 17
stackoverflow.com
. iOS 26 uses AVCaptureDevice.RotationCoordinator to handle orientation for photo capture. For video recording, this reinforces the approach of handling orientation in post-processing or via videoRotationAngle. Our manual rotation method will continue to work since it doesn't rely on deprecated properties. The app now targets iOS 26, which introduces `effectiveGeometry.interfaceOrientation` as the replacement for the deprecated `windowScene.interfaceOrientation`. If you use any orientation APIs, ensure you migrate to the newer ones to avoid warnings or potential no-ops.
AVCaptureMultiCamSession improvements: Each iOS release may expand multi-cam capabilities. With iOS 26, Apple's own Camera app supports Dual Capture (front/back) natively, so the API is mature. Keep an eye on documentation for any new multi-cam features – for instance, support for more than two simultaneous cameras, higher resolution streams, or better power management. Ensure you test on the latest devices, as the set of supported multi-cam device combinations can change. (For example, a future device might allow three cameras simultaneously, or an iPad might allow an external camera plus internal cameras in a session
developer.apple.com
.)
Media pipeline changes: Check for any changes in AVFoundation around buffer formats and color spaces. For instance, if Apple were to introduce wider color gamut video capture by default, you would want to explicitly handle color space (perhaps using P3 color space for CIContext if needed). In our current setup, we explicitly use sRGB, which is fine for standard video. If HDR (High Dynamic Range) capture is enabled, the pixel buffer format might be 10-bit YUV, requiring matching pixel format keys and possibly a different color space (Rec.2020). Always query the format of AVCaptureDevice.activeFormat and adjust settings accordingly for new formats.
Best practices recap: Continue to use pixel buffer pools and reuse buffers to avoid memory churn. Avoid block-based pixel buffer creation inside the capture loop; instead create a pool up front (via the adaptor) and draw into that. Manage thread synchronization carefully – the sample uses locks around the lastFrontBuffer cache and a flag for shutdown. This kind of careful design will be even more important if Apple enables higher frame rates or more streams. Using @actors (as done with the RecordingCoordinator actor) is a good way to isolate threading issues.
Testing on all devices: “Consistent behavior across devices” means you should test on various iPhone models and iOS versions. Pay attention to older devices (which might have lower throughput – ensure your code checks isMultiCamSupported and possibly restricts frame size/FPS). Also, different lenses have different aspect ratios (some front cameras might capture 1440×1920 instead of 1080×1920). The code should dynamically use the actual captured buffer size to compute the portrait dimensions rather than hard-coding values. In our example, if portraitWidth and portraitHeight are determined from the capture format, the solution will automatically adapt to any slight differences.
By following these practices and being aware of iOS API changes, the dual-camera app will reliably record front, back, and combined videos in portrait orientation without layout issues or green artifacts. The implementation choices (using AVCaptureMultiCamSession, proper AVAssetWriter setup, Core Image compositing with color management, and pipeline flushing) collectively resolve the problems and ensure high-quality output for all supported devices.
stackoverflow.com
Citations

ios - iPhone 4 AVFoundation : Capture from front and rear cameras simultaneously - Stack Overflow

https://stackoverflow.com/questions/4807619/iphone-4-avfoundation-capture-from-front-and-rear-cameras-simultaneously

